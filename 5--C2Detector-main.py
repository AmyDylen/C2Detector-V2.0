import os
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from functools import partial
import datetime
import sys
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨ç±»
class Logger:
    def __init__(self, log_file_path):
        self.log_file_path = log_file_path
        self.terminal = sys.stdout
        self.log_file = open(log_file_path, 'w', encoding='utf-8')
        
    def write(self, message):
        self.terminal.write(message)
        self.log_file.write(message)
        self.log_file.flush()
        
    def flush(self):
        self.terminal.flush()
        self.log_file.flush()
        
    def close(self):
        self.log_file.close()

# è®¾å¤‡é…ç½®å‡½æ•°
def get_device():
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
    else:
        device = torch.device('cpu')
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
    return device
        
# æ•°æ®åŠ è½½æ¨¡å—
class CustomDataset(Dataset):
    def __init__(self, data_root, max_seq_len=50, 
                min_samples=10, max_samples_per_class=None):
        """
        å‚æ•°è¯´æ˜ï¼š
        min_samples: int - æ¯ä¸ªç±»åˆ«æœ€å°æ ·æœ¬æ•° (é»˜è®¤10)
        max_samples_per_class: int - æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºå¿«é€Ÿè®­ç»ƒ (Noneè¡¨ç¤ºä¸é™åˆ¶)
        """
        self.sequences = []
        self.labels = []
        self.sample_info = []  # æ–°å¢ï¼šå­˜å‚¨æ ·æœ¬æ¥æºä¿¡æ¯
        self.max_seq_len = max_seq_len
        self.max_samples_per_class = max_samples_per_class
        
        # æŒ‰ç±»åˆ«æ”¶é›†æ•°æ®
        class_data = {}  # {class_idx: [(seq_data, label, info), ...]}
        sample_info_data = {}  # {class_idx: [sample_info, ...]}

        # éå†æ–‡ä»¶å¤¹ç»“æ„
        for class_idx, class_name in enumerate(sorted(os.listdir(data_root))):
            class_dir = os.path.join(data_root, class_name)
            if not os.path.isdir(class_dir):
                continue
                
            class_data[class_idx] = []
            sample_info_data[class_idx] = []
                
            # å¤„ç†æ¯ä¸ªç±»åˆ«çš„CSVæ–‡ä»¶
            for csv_file in os.listdir(class_dir):
                if not csv_file.endswith('.csv') or 'UDP' in csv_file:
                    continue
                    
                file_path = os.path.join(class_dir, csv_file)
                try:
                    # è¯»å–å¹¶é¢„å¤„ç†æ•°æ®
                    df = pd.read_csv(file_path)
                    
                    # æ–°å¢åˆ—åæ ¡éªŒ
                    required_columns = {'Session ID', 'direction1', 'size1', 'time1',
                                      'direction2', 'size2', 'time2', 'flag', 'ratio', 'diff'}
                    if not required_columns.issubset(df.columns):
                        print(f"æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…è¦åˆ—ï¼Œå·²è·³è¿‡")
                        continue
                        
                    # æŒ‰Session IDåˆ†ç»„å¤„ç†
                    for session_id, group in df.groupby('Session ID'):
                        # æå–ç‰¹å¾æ•°æ®
                        seq_data = group[['direction1', 'size1', 'time1',
                                       'direction2', 'size2', 'time2',
                                       'flag', 'ratio', 'diff']].values.astype(np.float32)
                        
                        # ä¿®æ”¹åçš„åºåˆ—åˆ†å‰²é€»è¾‘
                        sequence_length = len(seq_data)
                        
                        # åŸå§‹é•¿åº¦æœ‰æ•ˆæ€§æ£€æŸ¥
                        if sequence_length < 1:
                            continue
                            
                        # å­˜å‚¨æ ·æœ¬ä¿¡æ¯å’Œæ¥æº
                        sample_info = {
                            'file_path': file_path,
                            'csv_file': csv_file,
                            'class_name': class_name
                        }
                        
                        class_data[class_idx].append((seq_data, class_idx))
                        sample_info_data[class_idx].append(sample_info)
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¯¥ç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
                        if self.max_samples_per_class and len(class_data[class_idx]) >= self.max_samples_per_class:
                            break
                        
                except Exception as e:
                    print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    continue
            
            # å¦‚æœè¯¥ç±»åˆ«è®¾ç½®äº†æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ï¼Œè¿›è¡Œéšæœºé‡‡æ ·
            if self.max_samples_per_class and len(class_data[class_idx]) > self.max_samples_per_class:
                print(f"ç±»åˆ« {class_idx} åŸå§‹æ ·æœ¬æ•°: {len(class_data[class_idx])}, é‡‡æ ·åˆ°: {self.max_samples_per_class}")
                # éšæœºé‡‡æ ·
                sampled_indices = np.random.choice(len(class_data[class_idx]), 
                                                 self.max_samples_per_class, replace=False)
                class_data[class_idx] = [class_data[class_idx][i] for i in sampled_indices]
                sample_info_data[class_idx] = [sample_info_data[class_idx][i] for i in sampled_indices]
        
        # å°†æ‰€æœ‰ç±»åˆ«çš„æ•°æ®åˆå¹¶
        for class_idx, data_list in class_data.items():
            self.sequences.extend([item[0] for item in data_list])
            self.labels.extend([item[1] for item in data_list])
            self.sample_info.extend(sample_info_data[class_idx])
                    
        # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼Œå¦‚æœå°äºmin_samplesåˆ™æŠ¥é”™
        self._check_min_samples(min_samples)
        
        # æœ€ç»ˆæ•°æ®æ£€æŸ¥
        print(f"\næ•°æ®åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°ï¼š{len(self.sequences)}")
        if len(self.sequences) == 0:
            raise RuntimeError("æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼")
            
    def _check_min_samples(self, min_samples):
        """æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚"""
        # æŒ‰ç±»åˆ«æ”¶é›†æ ·æœ¬ç´¢å¼•
        class_indices = {}
        for idx, label in enumerate(self.labels):
            if label not in class_indices:
                class_indices[label] = []
            class_indices[label].append(idx)
        
        print(f"\næ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼Œæœ€å°æ ·æœ¬æ•°è¦æ±‚: {min_samples}")
        print("ç±»åˆ«åˆ†å¸ƒï¼š")
        for cls, indices in class_indices.items():
            count = len(indices)
            print(f"ç±»åˆ« {cls}: {count} æ ·æœ¬")
            
            # å¦‚æœæ ·æœ¬æ•°å°äºæœ€å°è¦æ±‚ï¼Œç›´æ¥æŠ¥é”™
            if count < min_samples:
                raise ValueError(f"ç±»åˆ« {cls} æ ·æœ¬æ•°ä¸è¶³ï¼š{count} < {min_samples}ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        
        print(f"âœ“ æ‰€æœ‰ç±»åˆ«æ ·æœ¬æ•°å‡æ»¡è¶³æœ€å°è¦æ±‚ (â‰¥ {min_samples})")
        
    def __len__(self):
        return len(self.labels)
        
    def __getitem__(self, idx):
        return {
            'features': self.sequences[idx],
            'label': torch.tensor(self.labels[idx], dtype=torch.long),
            'sample_info': self.sample_info[idx]  # æ–°å¢ï¼šè¿”å›æ ·æœ¬ä¿¡æ¯
        }

# ä¿®æ”¹åçš„æ¨¡å‹å®šä¹‰
class HierarchicalAttentionClassifier(nn.Module):
    def __init__(self, d_model=96, n_classes=5):
        super().__init__()
        self.d_model = d_model
        
        self.time_thresholds =    [1000,10000, 100000, 1000000, 5000000,  10000000, 30000000, 60000000]
        self.time_bucket_widths = [10,  100,   100,    1000,    2000,    100000,   1000000,   2000000]
        time_total_buckets = self._calculate_total_buckets(self.time_thresholds, self.time_bucket_widths)
        
        self.size_thresholds = [10000, 100000, 1000000]
        self.size_bucket_widths = [1,  100,  10000]
        size_total_buckets = self._calculate_total_buckets(self.size_thresholds, self.size_bucket_widths)
        
        self.ratio_thresholds =    [1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, 10000000000, 100000000000, 1000000000000]
        self.ratio_bucket_widths = [100,  100,   100,    1000,    10000,    100000,   1000000,    100000000,   10000000000,  100000000000]
        ratio_total_buckets = self._calculate_total_buckets(self.ratio_thresholds, self.ratio_bucket_widths)
        
        self.diff_thresholds =    [1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, 10000000000, 100000000000, 6000000000000]
        self.diff_bucket_widths = [100,   100,  100,    1000,    10000,    100000,    1000000,    100000000,   10000000000,  100000000000]
        diff_total_buckets = self._calculate_total_buckets(self.diff_thresholds, self.diff_bucket_widths)
        
        # åˆå§‹åŒ–åµŒå…¥å±‚
        self.dir_emb = nn.Embedding(2, d_model)
        self.size_emb = nn.Embedding(size_total_buckets, d_model)
        self.time_emb = nn.Embedding(time_total_buckets, d_model)
        self.flag_emb = nn.Embedding(2, d_model)
        # å¯¹æ•°åˆ†ç®±
        self.ratio_emb = nn.Embedding(ratio_total_buckets, d_model) 
        self.diff_emb = nn.Embedding(diff_total_buckets, d_model)
        
        # ç‰¹å¾å¤„ç†å™¨ï¼ˆå‰ä¸¤ç»„å¸¦æ³¨æ„åŠ›ï¼Œç¬¬ä¸‰ç»„ç›´æ¥æŠ•å½±ï¼‰
        self.group_processors = nn.ModuleList([
            # å‰ä¸¤ç»„å¤„ç†å™¨
            nn.Sequential(
                nn.MultiheadAttention(d_model, 4, batch_first=True),
                nn.LayerNorm(d_model),
                nn.Linear(d_model, d_model*2),
                nn.GELU(),
                nn.Linear(d_model*2, d_model*2)
            ) for _ in range(2)
        ] + [
            # ç¬¬ä¸‰ç»„å¤„ç†å™¨
            nn.Sequential(
                nn.Linear(d_model, d_model*2),
                nn.GELU(),
                nn.LayerNorm(d_model*2)
            )
        ])
        
        # è·¨ç»„æ³¨æ„åŠ›å±‚
        self.cross_attn = nn.MultiheadAttention(d_model*2, 4, batch_first=True)
        
        # ä¿®æ­£å·ç§¯å±‚è¾“å…¥ç»´åº¦
        self.conv_block = nn.Sequential(
            nn.Conv1d(2*(d_model*2) + (d_model*2), 512, 5, padding=2),  # å®é™…è¾“å…¥ç»´åº¦
            nn.GELU(),
            nn.BatchNorm1d(512),
            nn.AdaptiveMaxPool1d(100),
            nn.Dropout(0.2)
        )
        
        # å…¨å±€Transformer
        self.global_transformer = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(
                d_model=512,
                nhead=8,
                dim_feedforward=2048,
                batch_first=True
            ),
            num_layers=2
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.SiLU(),
            nn.Dropout(0.3),
            nn.Linear(256, n_classes)
        )
        
    def forward(self, x, mask=None):
        # ç‰¹å¾åˆ†ç»„å¤„ç†
        B = x.size(0)  # batch size
        
        # ç¬¬ä¸€ç»„ï¼ˆæ–¹å‘/å°ºå¯¸/æ—¶é—´ï¼‰
        group1 = torch.stack([
            self.dir_emb(self._safe_index(x[:,:,0])),
            self.size_emb(self._log_bucketize(x[:,:,1])),
            self.time_emb(self._time_bucketize(x[:,:,2]))
        ], dim=2)
        
        # ç¬¬äºŒç»„ï¼ˆæ–¹å‘/å°ºå¯¸/æ—¶é—´ï¼‰
        group2 = torch.stack([
            self.dir_emb(self._safe_index(x[:,:,3])),
            self.size_emb(self._log_bucketize(x[:,:,4])),
            self.time_emb(self._time_bucketize(x[:,:,5]))
        ], dim=2)
        
        # ç¬¬ä¸‰ç»„ï¼ˆç»„åˆå±æ€§ï¼‰
        group3 = torch.stack([
            self.flag_emb(self._flag_bucketize(x[:,:,6])),
            self.ratio_emb(self._ratio_bucketize(x[:,:,7])),
            self.diff_emb(self._diff_bucketize(x[:,:,8]))
        ], dim=2)
        
        # ç»„å†…å¤„ç†
        g1 = self._process_group(group1, 0)  # [B, S, d_model*2]
        g2 = self._process_group(group2, 1)  # [B, S, d_model*2]
        g3 = self._process_group(group3, 2)  # [B, S, d_model*2]
        
        # è·¨ç»„æ³¨æ„åŠ›ï¼ˆä»…å‰ä¸¤ç»„ï¼‰
        attn_out, _ = self.cross_attn(g1, g2, g2, key_padding_mask=mask)
        
        # æ­£ç¡®çš„ç‰¹å¾æ‹¼æ¥æ–¹å¼
        final_combined = torch.cat([g1, g2, g3], dim=-1)  # [B, S, 3*d_model*2]
        
        # ç»´åº¦è°ƒæ•´
        conv_input = final_combined.transpose(1, 2)  # [B, 3*d_model*2, S]
        conv_feat = self.conv_block(conv_input)      # [B, 512, 100]
        
        # å…¨å±€Transformerå¤„ç†
        trans_input = conv_feat.transpose(1, 2)  # [B, 100, 512]
        trans_out = self.global_transformer(trans_input)  # [B, 100, 512]
        
        # åˆ†ç±»å†³ç­–
        aggregated, _ = torch.max(trans_out, dim=1)  # [B, 512]
        return self.classifier(aggregated)  # [B, n_classes]
        
    def _process_group(self, group, processor_id):
        """ä¿®æ”¹åçš„ç»„å¤„ç†é€»è¾‘"""
        B, S = group.shape[0], group.shape[1]
        reshaped = group.view(B*S, 3, self.d_model)  # [B*S, 3, d_model]
        
        if processor_id < 2:  # å‰ä¸¤ç»„ä½¿ç”¨æ³¨æ„åŠ›
            # æ³¨æ„åŠ›å¤„ç†
            attn_out, _ = self.group_processors[processor_id][0](reshaped, reshaped, reshaped)
            norm_out = self.group_processors[processor_id][1](attn_out)
            
            # ç‰¹å¾èšåˆ
            aggregated = norm_out.mean(dim=1)  # [B*S, d_model]
            
            # ç»´åº¦æ‰©å±•
            proj_out = self.group_processors[processor_id][2](aggregated)  # [B*S, d_model*2]
        else:  # ç¬¬ä¸‰ç»„ç›´æ¥å¤„ç†
            # å¹³å‡èšåˆ
            aggregated = reshaped.mean(dim=1)  # [B*S, d_model]
            
            # ç›´æ¥æŠ•å½±
            proj_out = self.group_processors[processor_id][0](aggregated)  # [B*S, d_model*2]
            proj_out = self.group_processors[processor_id][2](proj_out)  # LayerNorm
        
        return proj_out.view(B, S, -1)  # [B, S, d_model*2]
    
    # æ·»åŠ è®¡ç®—æ€»æ¡¶æ•°é‡çš„æ–¹æ³•
    def _calculate_total_buckets(self, thresholds, bucket_widths, min_value = 1):
        start_buckets = [0]
        # è®¡ç®—æ¯ä¸ªåŒºé—´çš„èµ·å§‹æ¡¶ç¼–å·
        for i in range(1, len(thresholds)):
            if i == 1:
                range_start = min_value
                range_end = thresholds[i-1]
            else:
                range_start = thresholds[i-2] + 1
                range_end = thresholds[i-1]
            
            bucket_count = ((range_end - range_start) // bucket_widths[i-1]) + 1
            bucket_count = max(1, bucket_count)
            next_start = start_buckets[-1] + bucket_count
            start_buckets.append(next_start)
        
        # è®¡ç®—æœ€åä¸€ä¸ªåŒºé—´çš„æ¡¶æ•°é‡
        last_range_start = thresholds[-2] + 1 if len(thresholds) > 1 else min_value
        last_range_end = thresholds[-1]
        last_bucket_count = ((last_range_end - last_range_start) // bucket_widths[-1]) + 1
        
        return start_buckets[-1] + max(1, last_bucket_count)
        
    def _auto_dynamic_range_bucketize(self, values, thresholds, bucket_widths, min_value=1):
        """è‡ªåŠ¨åŠ¨æ€åŒºé—´ç¼–ç ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šèµ·å§‹æ¡¶ç¼–å·
        
        å‚æ•°:
            values: è¾“å…¥çš„æ—¶é—´å€¼å¼ é‡
            thresholds: åŒºé—´è¾¹ç•Œåˆ—è¡¨ï¼Œå‡åºæ’åˆ—
            bucket_widths: æ¯ä¸ªåŒºé—´çš„æ¡¶å®½åº¦åˆ—è¡¨
            min_value: æœ€å°å€¼ï¼Œé»˜è®¤ä¸º1
            
        è¿”å›:
            bucketed: ç¼–ç åçš„æ¡¶ç´¢å¼•å¼ é‡
        """
        # ç¡®ä¿thresholdså’Œbucket_widthsé•¿åº¦ä¸€è‡´
        assert len(thresholds) == len(bucket_widths), "é˜ˆå€¼å’Œæ¡¶å®½åº¦æ•°é‡å¿…é¡»ä¸€è‡´"
        
        # è®¡ç®—æ¯ä¸ªåŒºé—´çš„èµ·å§‹æ¡¶ç¼–å·
        start_buckets = [0]  # ç¬¬ä¸€ä¸ªåŒºé—´çš„èµ·å§‹æ¡¶ç¼–å·ä¸º0
        
        # è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ¡¶æ•°é‡å¹¶ç´¯åŠ å¾—åˆ°èµ·å§‹æ¡¶ç¼–å·
        for i in range(1, len(thresholds)):
            # è®¡ç®—å½“å‰åŒºé—´çš„æ•°å€¼èŒƒå›´
            if i == 1:
                # ç¬¬ä¸€ä¸ªåŒºé—´ï¼šä»æœ€å°å€¼åˆ°ç¬¬ä¸€ä¸ªé˜ˆå€¼
                range_start = min_value
                range_end = thresholds[i-1]
            else:
                # ä¸­é—´åŒºé—´ï¼šä»ä¸Šä¸€ä¸ªé˜ˆå€¼åˆ°å½“å‰é˜ˆå€¼
                range_start = thresholds[i-2] + 1  # +1é¿å…é‡å 
                range_end = thresholds[i-1]
            
            # è®¡ç®—è¯¥åŒºé—´çš„æ¡¶æ•°é‡
            bucket_count = ((range_end - range_start) // bucket_widths[i-1]) + 1
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ¡¶
            bucket_count = max(1, bucket_count)
            # è®¡ç®—ä¸‹ä¸€ä¸ªåŒºé—´çš„èµ·å§‹æ¡¶ç¼–å·
            next_start = start_buckets[-1] + bucket_count
            start_buckets.append(next_start)
        
        # åˆå§‹åŒ–ç»“æœå¼ é‡
        bucketed = torch.zeros_like(values, dtype=torch.long)
        
        # å¯¹æ¯ä¸ªåŒºé—´åº”ç”¨ä¸åŒçš„æ¡¶åŒ–ç­–ç•¥
        for i in range(len(thresholds)):
            if i == 0:
                # ç¬¬ä¸€ä¸ªåŒºé—´ï¼šå°äºç­‰äºç¬¬ä¸€ä¸ªé˜ˆå€¼
                mask = values <= thresholds[i]
            elif i == len(thresholds) - 1:
                # æœ€åä¸€ä¸ªåŒºé—´ï¼šå¤§äºä¸Šä¸€ä¸ªé˜ˆå€¼
                mask = values > thresholds[i-1]
            else:
                # ä¸­é—´åŒºé—´ï¼šå¤§äºä¸Šä¸€ä¸ªé˜ˆå€¼ä¸”å°äºç­‰äºå½“å‰é˜ˆå€¼
                mask = (values > thresholds[i-1]) & (values <= thresholds[i])
            
            if mask.any():
                if i == 0:
                    # ç¬¬ä¸€ä¸ªåŒºé—´ï¼šä»æœ€å°å€¼å¼€å§‹è®¡ç®—
                    bucket_values = (values[mask] - min_value) // bucket_widths[i]
                else:
                    # å…¶ä»–åŒºé—´ï¼šä»ä¸Šä¸€ä¸ªé˜ˆå€¼+1å¼€å§‹è®¡ç®—
                    bucket_values = (values[mask] - (thresholds[i-1] + 1)) // bucket_widths[i]
                
                # ç¡®ä¿æ¡¶ç´¢å¼•éè´Ÿ
                bucket_values = torch.clamp(bucket_values, 0)
                # è®¡ç®—æœ€ç»ˆæ¡¶ç´¢å¼•å¹¶ç¡®ä¿ç±»å‹åŒ¹é…
                bucketed[mask] = (start_buckets[i] + bucket_values).long()
        
        # ç¡®ä¿æœ€ç»ˆç´¢å¼•ä¸è¶…è¿‡Embeddingå±‚å¤§å°
        return torch.clamp(bucketed, 0, self.time_emb.num_embeddings - 1)
    
    # è¾…åŠ©æ–¹æ³•ä¿æŒä¸å˜
    def _safe_index(self, tensor):
        return torch.clamp(((tensor + 1) // 2).long(), 0, 1)
    
    def _log_bucketize(self, values):
        valid_values = torch.clamp(values, 1, 999999)
        bucketed = self._auto_dynamic_range_bucketize(valid_values, self.size_thresholds, self.size_bucket_widths, min_value=1)
        return bucketed
    
    def _time_bucketize(self, values):
    # é¦–å…ˆç¡®ä¿è¾“å…¥æ˜¯æœ‰æ•ˆçš„,ç„¶ååº”ç”¨æ¡¶åŒ–ç­–ç•¥
        valid_values = torch.clamp(values, 1, 59999999)
        bucketed = self._auto_dynamic_range_bucketize(valid_values, self.time_thresholds, self.time_bucket_widths, min_value=1)
        return bucketed
    
    def _flag_bucketize(self, values):
        return torch.clamp(values.long(), 0, 1)
    
    def _ratio_bucketize(self, values):
        valid_values = torch.clamp(values*1000000, 1, 59999999)
        bucketed = self._auto_dynamic_range_bucketize(valid_values, self.ratio_thresholds, self.ratio_bucket_widths, min_value=1)
        return bucketed
    
    def _diff_bucketize(self, values):
        valid_values = torch.clamp(values*100000, 1, 59999999)
        bucketed = self._auto_dynamic_range_bucketize(valid_values, self.diff_thresholds, self.diff_bucket_widths, min_value=1)
        return bucketed  # 
    
# æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒæ¨¡å—ï¼ˆä¿®å¤collate_fnå‡½æ•°ï¼‰
def collate_fn(batch, max_seq_len=50):
    features = [item['features'] for item in batch]
    labels = torch.stack([item['label'] for item in batch])
    
    # æ–°å¢ï¼šæå–sample_infoå­—æ®µ
    sample_infos = []
    for item in batch:
        if 'sample_info' in item:
            sample_infos.append(item['sample_info'])
        else:
            sample_infos.append(None)
    
    processed_features = []
    masks = []
    for f in features:
        seq_len = f.shape[0]
        if seq_len > max_seq_len:
            truncated = f[:max_seq_len]
            mask = torch.zeros(max_seq_len).bool()
        else:
            truncated = np.zeros((max_seq_len, 9), dtype=np.float32)
            truncated[:seq_len] = f
            mask = torch.cat([torch.zeros(seq_len), 
                            torch.ones(max_seq_len - seq_len)]).bool()
        processed_features.append(torch.FloatTensor(truncated))
        masks.append(mask)
    
    return {
        'features': torch.stack(processed_features),
        'label': labels,
        'mask': torch.stack(masks),
        'sample_info': sample_infos  # æ–°å¢ï¼šä¼ é€’æ ·æœ¬ä¿¡æ¯
    }
    
    
def train_model(data_root, split_params=None):
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = f"training_and_Test_log_{timestamp}.txt"
    
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = Logger(log_file_path)
    sys.stdout = logger
    
    try:
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - {timestamp}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
        print("=" * 80)
        
        # è·å–è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
        device = get_device()
                 
        config = {
            'batch_size': 32,
            'lr': 1e-4,
            'epochs': 100,
            'max_seq_len': 50,
            'd_model': 128,
            # é»˜è®¤å‚æ•°
            'min_samples': 10,
            'patience': 5,
            'save_best_model': True,
            # è®­ç»ƒé›†åˆ’åˆ†æ¯”ä¾‹ï¼ˆä»trainæ–‡ä»¶å¤¹ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†ï¼‰
            'train_ratio': 0.9,    # 90%è®­ç»ƒé›†ï¼ˆä»trainæ–‡ä»¶å¤¹ä¸­ï¼‰
            'val_ratio': 0.1,      # 10%éªŒè¯é›†ï¼ˆä»trainæ–‡ä»¶å¤¹ä¸­ï¼‰
            'max_samples_per_class': 10000,  # æ–°å¢ï¼šæ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°
            'use_amp': True,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            'num_workers': 4 if torch.cuda.is_available() else 0,  # DataLoaderå·¥ä½œè¿›ç¨‹æ•°
        }
        
        # åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰å‚æ•°
        if split_params:
            config.update(split_params)
        
        # æ„å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†è·¯å¾„
        train_dir = os.path.join(data_root, 'train')
        test_dir = os.path.join(data_root, 'test')
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_dir):
            raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {train_dir}")
        if not os.path.exists(test_dir):
            raise FileNotFoundError(f"æµ‹è¯•é›†æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {test_dir}")
        
        print(f"æ•°æ®è·¯å¾„:")
        print(f"  è®­ç»ƒé›†: {train_dir}")
        print(f"  æµ‹è¯•é›†: {test_dir}")
        
        # åŠ è½½è®­ç»ƒé›†
        try:
            print("\næ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
            train_dataset = CustomDataset(
                train_dir,
                max_seq_len=config['max_seq_len'],
                min_samples=config['min_samples'],
                max_samples_per_class=config['max_samples_per_class']
            )
            print(f"è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…± {len(train_dataset)} æ ·æœ¬")
        except Exception as e:
            print(f"è®­ç»ƒé›†åŠ è½½å¤±è´¥: {str(e)}")
            return
        
        # åŠ è½½æµ‹è¯•é›†
        try:
            print("\næ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
            test_dataset = CustomDataset(
                test_dir,
                max_seq_len=config['max_seq_len'],
                min_samples=config['min_samples']
            )
            print(f"æµ‹è¯•é›†åŠ è½½å®Œæˆï¼Œå…± {len(test_dataset)} æ ·æœ¬")
        except Exception as e:
            print(f"æµ‹è¯•é›†åŠ è½½å¤±è´¥: {str(e)}")
            return
        
        # éªŒè¯æ•°æ®é›†
        n_classes = len(np.unique(train_dataset.labels))
        print(f"\næ£€æµ‹åˆ° {n_classes} ä¸ªæœ‰æ•ˆç±»åˆ«")
        
        if n_classes < 2:
            raise ValueError(f"è‡³å°‘éœ€è¦2ä¸ªç±»åˆ«ï¼Œå½“å‰æ£€æµ‹åˆ°{n_classes}ä¸ª")
        
        # æ£€æŸ¥è®­ç»ƒé›†æ ·æœ¬æ€»æ•°
        total_train_samples = len(train_dataset)
        if total_train_samples < 5:
            raise ValueError(
                f"è®­ç»ƒé›†éœ€è¦è‡³å°‘5ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå½“å‰åªæœ‰{total_train_samples}ä¸ªæ ·æœ¬ã€‚"
            )
        
        # æ£€æŸ¥æµ‹è¯•é›†æ ·æœ¬æ€»æ•°
        total_test_samples = len(test_dataset)
        if total_test_samples < 1:
            raise ValueError(
                f"æµ‹è¯•é›†éœ€è¦è‡³å°‘1ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œå½“å‰åªæœ‰{total_test_samples}ä¸ªæ ·æœ¬ã€‚"
            )
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        print(f"\næ•°æ®é›†ä¿¡æ¯:")
        print(f"  è®­ç»ƒé›†æ€»æ ·æœ¬æ•°: {total_train_samples}")
        print(f"  æµ‹è¯•é›†æ€»æ ·æœ¬æ•°: {total_test_samples}")
        
        # æ£€æŸ¥è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
        unique_train_labels, train_counts = np.unique(train_dataset.labels, return_counts=True)
        print("\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
        for label, count in zip(unique_train_labels, train_counts):
            print(f"  ç±»åˆ« {label}: {count} æ ·æœ¬")
        
        # æ£€æŸ¥æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ
        unique_test_labels, test_counts = np.unique(test_dataset.labels, return_counts=True)
        print("\næµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒï¼š")
        for label, count in zip(unique_test_labels, test_counts):
            print(f"  ç±»åˆ« {label}: {count} æ ·æœ¬")
        
        # ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†
        try:
            train_idx, val_idx = train_test_split(
                np.arange(total_train_samples),
                test_size=config['val_ratio'],
                stratify=train_dataset.labels if total_train_samples > 10 else None,
                random_state=42
            )
        except ValueError as e:
            # åˆ†å±‚æŠ½æ ·å¤±è´¥æ—¶æ”¹ç”¨ç®€å•æŠ½æ ·
            print("åˆ†å±‚æŠ½æ ·å¤±è´¥ï¼Œä½¿ç”¨ç®€å•éšæœºæŠ½æ ·")
            train_idx, val_idx = train_test_split(
                np.arange(total_train_samples),
                test_size=config['val_ratio'],
                random_state=42
            )
        
        print(f"\nè®­ç»ƒé›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬ ({len(train_idx)/total_train_samples:.1%})")
        print(f"  éªŒè¯é›†: {len(val_idx)} æ ·æœ¬ ({len(val_idx)/total_train_samples:.1%})")
        print(f"  æµ‹è¯•é›†: {total_test_samples} æ ·æœ¬ (ç‹¬ç«‹æµ‹è¯•é›†)")
        
        # åˆ›å»ºæ•°æ®å­é›†
        train_set = torch.utils.data.Subset(train_dataset, train_idx)
        val_set = torch.utils.data.Subset(train_dataset, val_idx)
        # æµ‹è¯•é›†ç›´æ¥ä½¿ç”¨å®Œæ•´çš„test_dataset
        
        # åˆ›å»ºDataLoaderï¼ˆæ·»åŠ GPUåŠ é€Ÿç›¸å…³å‚æ•°ï¼‰
        collate = partial(collate_fn, max_seq_len=config['max_seq_len'])
        train_loader = DataLoader(
            train_set, 
            batch_size=config['batch_size'], 
            shuffle=True, 
            collate_fn=collate,
            num_workers=config['num_workers'],
            pin_memory=True if torch.cuda.is_available() else False,
            persistent_workers=True if config['num_workers'] > 0 else False
        )
        val_loader = DataLoader(
            val_set, 
            batch_size=config['batch_size'],
            collate_fn=collate,
            num_workers=config['num_workers'],
            pin_memory=True if torch.cuda.is_available() else False,
            persistent_workers=True if config['num_workers'] > 0 else False
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=config['batch_size'],
            collate_fn=collate,
            num_workers=config['num_workers'],
            pin_memory=True if torch.cuda.is_available() else False,
            persistent_workers=True if config['num_workers'] > 0 else False
        )
        
        # åˆå§‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        model = HierarchicalAttentionClassifier(config['d_model'], n_classes=n_classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=config['lr'])
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
        scaler = GradScaler() if config['use_amp'] and torch.cuda.is_available() else None
        if scaler:
            print(f"âœ“ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        
        # æ—©åœæœºåˆ¶ç›¸å…³å˜é‡
        best_val_accuracy = 0.0
        best_model_state = None
        patience_counter = 0
        best_epoch = 0
        
        print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {config['epochs']} è½®ï¼Œæ—©åœè€å¿ƒå€¼: {config['patience']}")
        print("=" * 80)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(config['epochs']):
            model.train()
            total_loss = 0
            batch_count = 0
            
            # è®­ç»ƒé˜¶æ®µ
            for batch in train_loader:
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                features = batch['features'].to(device, non_blocking=True)
                labels = batch['label'].to(device, non_blocking=True)
                mask = batch['mask'].to(device, non_blocking=True)
                
                optimizer.zero_grad()
                
                # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                if scaler:
                    with autocast():
                        outputs = model(features, mask)
                        loss = criterion(outputs, labels)
                    
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(features, mask)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            avg_loss = total_loss / batch_count if batch_count > 0 else 0
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼ˆä¸æ˜¯æµ‹è¯•é›†ï¼ï¼‰
            val_metrics = evaluate(model, val_loader, device)
            current_val_accuracy = val_metrics['accuracy']
            
            # æ—©åœé€»è¾‘ï¼šæ£€æŸ¥éªŒè¯ç²¾åº¦æ˜¯å¦æå‡
            if current_val_accuracy > best_val_accuracy:
                best_val_accuracy = current_val_accuracy
                best_model_state = model.state_dict().copy()
                best_epoch = epoch + 1
                patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if config['save_best_model']:
                    current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    model_save_path = f"best_model_epoch_{epoch+1}_val_acc_{best_val_accuracy:.4f}_{current_time}.pth"
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'val_accuracy': best_val_accuracy,
                        'config': config,
                        'n_classes': n_classes,
                        'train_idx': train_idx,
                        'val_idx': val_idx,
                        'test_dataset_size': len(test_dataset)
                    }, model_save_path)
                    print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_save_path}")
            else:
                patience_counter += 1
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            print(f"Epoch {epoch+1:3d}/{config['epochs']} | "
                f"Loss: {avg_loss:.4f} | "
                f"Val Acc: {val_metrics['accuracy']:.4f} | "
                f"Val P_mac: {val_metrics['precision_macro']:.4f} | "
                f"Val R_mac: {val_metrics['recall_macro']:.4f} | "
                f"Val F1_mac: {val_metrics['f1_macro']:.4f} | "
                f"Val FPR_mac: {val_metrics['fpr_macro']:.4f} | "
                f"Best Val Acc: {best_val_accuracy:.4f} (Epoch {best_epoch}) | "
                f"Patience: {patience_counter}/{config['patience']}")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= config['patience']:
                print(f"\nâš¡ æ—©åœè§¦å‘ï¼è¿ç»­ {config['patience']} è½®éªŒè¯ç²¾åº¦æ— æå‡")
                print(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_val_accuracy:.4f} (ç¬¬ {best_epoch} è½®)")
                break
        
        # è®­ç»ƒç»“æŸï¼Œæ¢å¤æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
        final_test_metrics = None
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼å·²æ¢å¤æœ€ä½³æ¨¡å‹ (ç¬¬ {best_epoch} è½®, éªŒè¯ç²¾åº¦: {best_val_accuracy:.4f})")
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼ˆä»æœªä½¿ç”¨è¿‡çš„æ•°æ®ï¼‰
            print("\nğŸ§ª åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
            final_test_metrics = evaluate(model, test_loader, device, return_details=True)
            
            # æ‰“å°æ··æ·†çŸ©é˜µ
            print_confusion_matrix(final_test_metrics['confusion_matrix'])
            
            # è°ƒç”¨é”™è¯¯åˆ†ç±»åˆ†æå‡½æ•°ï¼Œä¼ é€’æ¯ä¸ªç±»åˆ«çš„æ€»æ ·æœ¬æ•°
            if 'misclassified' in final_test_metrics:
                # ä»æ··æ·†çŸ©é˜µè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ€»æ ·æœ¬æ•°
                cm = final_test_metrics['confusion_matrix']
                total_samples_per_class = [cm[i, :].sum() for i in range(cm.shape[0])]
                
                analyze_misclassifications(final_test_metrics['misclassified'], 
                                        total_samples_per_class=total_samples_per_class)
            else:
                print("âš ï¸ æ²¡æœ‰é”™è¯¯åˆ†ç±»æ•°æ®å¯ç”¨")
            
            print("ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
            print(f"  å‡†ç¡®ç‡: {final_test_metrics['accuracy']:.4f}")
            print(f"  ç²¾ç¡®ç‡(å®): {final_test_metrics['precision_macro']:.4f}")
            print(f"  å¬å›ç‡(å®): {final_test_metrics['recall_macro']:.4f}")
            print(f"  F1åˆ†æ•°(å®): {final_test_metrics['f1_macro']:.4f}")
            print(f"  è¯¯æŠ¥ç‡(å®): {final_test_metrics['fpr_macro']:.4f}")
            print(f"  ç²¾ç¡®ç‡(å¾®): {final_test_metrics['precision_micro']:.4f}")
            print(f"  å¬å›ç‡(å¾®): {final_test_metrics['recall_micro']:.4f}")
            print(f"  F1åˆ†æ•°(å¾®): {final_test_metrics['f1_micro']:.4f}")
            print(f"  è¯¯æŠ¥ç‡(å¾®): {final_test_metrics['fpr_micro']:.4f}")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            # è·å–å½“å‰æ—¶é—´çš„æ—¶é—´æˆ³
            current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            final_model_path = f"final_model_test_acc_{final_test_metrics['accuracy']:.4f}_{current_time}.pth"
            torch.save({
                'epoch': best_epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_accuracy': best_val_accuracy,
                'test_accuracy': final_test_metrics['accuracy'],
                'config': config,
                'n_classes': n_classes,
                'train_idx': train_idx,
                'val_idx': val_idx,
                'test_dataset_size': len(test_dataset)
            }, final_model_path)
            print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        print("=" * 80)
        print("ğŸ‰ è®­ç»ƒæµç¨‹æ€»ç»“:")
        print(f"  â€¢ è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬")
        print(f"  â€¢ éªŒè¯é›†: {len(val_idx)} æ ·æœ¬")
        print(f"  â€¢ æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        print(f"  â€¢ æœ€ä½³éªŒè¯ç²¾åº¦: {best_val_accuracy:.4f} (ç¬¬ {best_epoch} è½®)")
        if final_test_metrics:
            print(f"  â€¢ æœ€ç»ˆæµ‹è¯•ç²¾åº¦: {final_test_metrics['accuracy']:.4f}")
            print(f"  â€¢ æ³›åŒ–å·®è·: {(best_val_accuracy - final_test_metrics['accuracy']):.4f}")
        
        return model, best_val_accuracy, final_test_metrics
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None
    finally:
        # æ¢å¤æ ‡å‡†è¾“å‡ºå¹¶å…³é—­æ—¥å¿—æ–‡ä»¶
        sys.stdout = logger.terminal
        logger.close()    
        
def evaluate(model, loader, device, return_details=False):
    model.eval()
    all_preds, all_labels = [], []
    all_probs = []
    sample_indices = []
    sample_infos = []  # æ–°å¢ï¼šå­˜å‚¨æ ·æœ¬ä¿¡æ¯
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            features = batch['features'].to(device, non_blocking=True)
            labels = batch['label'].to(device, non_blocking=True)
            mask = batch['mask'].to(device, non_blocking=True)
            
            outputs = model(features, mask)
            probs = torch.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)
            
            # ç§»å›CPUè¿›è¡Œåç»­å¤„ç†
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            
            # è®°å½•æ‰¹æ¬¡ç´¢å¼•å’Œæ ·æœ¬åœ¨æ‰¹æ¬¡ä¸­çš„ä½ç½®
            for i in range(len(labels)):
                sample_indices.append((batch_idx, i))
                # å­˜å‚¨æ ·æœ¬ä¿¡æ¯
                if 'sample_info' in batch:
                    sample_infos.append(batch['sample_info'][i])
                else:
                    sample_infos.append(None)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    n_classes = cm.shape[0]
    
    # è®¡ç®—è¯¯æŠ¥ç‡ (False Positive Rate)
    fpr_per_class = []
    for i in range(n_classes):
        fp = cm[:, i].sum() - cm[i, i]
        tn = cm.sum() - cm[i, :].sum() - cm[:, i].sum() + cm[i, i]
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        fpr_per_class.append(fpr)
    
    fpr_macro = np.mean(fpr_per_class)
    
    total_fp = 0
    total_tn = 0
    for i in range(n_classes):
        total_fp += cm[:, i].sum() - cm[i, i]
        total_tn += cm.sum() - cm[i, :].sum() - cm[:, i].sum() + cm[i, i]
    
    fpr_micro = total_fp / (total_fp + total_tn) if (total_fp + total_tn) > 0 else 0.0
    
    result = {
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision_macro': precision_score(all_labels, all_preds, average='macro', zero_division=0),
        'precision_micro': precision_score(all_labels, all_preds, average='micro', zero_division=0),
        'recall_macro': recall_score(all_labels, all_preds, average='macro', zero_division=0),
        'recall_micro': recall_score(all_labels, all_preds, average='micro', zero_division=0),
        'f1_macro': f1_score(all_labels, all_preds, average='macro', zero_division=0),
        'f1_micro': f1_score(all_labels, all_preds, average='micro', zero_division=0),
        'fpr_macro': fpr_macro,
        'fpr_micro': fpr_micro,
        'confusion_matrix': cm,
        'all_labels': all_labels,
        'all_preds': all_preds,
        'all_probs': all_probs,
        'sample_indices': sample_indices,
        'sample_infos': sample_infos  # æ–°å¢ï¼šæ ·æœ¬ä¿¡æ¯
    }
    
    if return_details:
        # åˆ†æé”™è¯¯åˆ†ç±»
        misclassified = []
        for i, (true_label, pred_label) in enumerate(zip(all_labels, all_preds)):
            if true_label != pred_label:
                misclassified.append({
                    'sample_index': i,
                    'batch_info': sample_indices[i],
                    'true_label': true_label,
                    'predicted_label': pred_label,
                    'confidence': all_probs[i][pred_label],
                    'true_class_confidence': all_probs[i][true_label],
                    'sample_info': sample_infos[i]  # æ–°å¢ï¼šæ ·æœ¬æ¥æºä¿¡æ¯
                })
        
        result['misclassified'] = misclassified
        result['misclassification_rate'] = len(misclassified) / len(all_labels)
    
    return result
  
def print_confusion_matrix(cm, class_names=None):
    """æ‰“å°æ ¼å¼åŒ–çš„æ··æ·†çŸ©é˜µ"""
    n_classes = cm.shape[0]
    if class_names is None:
        class_names = [f'Class {i}' for i in range(n_classes)]
    
    print("\nğŸ“Š æ··æ·†çŸ©é˜µ:")
    print(" " * 8, end="")
    for name in class_names:
        print(f"{name:>8}", end="")
    print("  (é¢„æµ‹)")
    
    for i in range(n_classes):
        print(f"{class_names[i]:>8}", end="")
        for j in range(n_classes):
            print(f"{cm[i, j]:>8}", end="")
        print()
    
    # æ‰“å°æ¯è¡Œçš„ç™¾åˆ†æ¯”
    print("\nğŸ“ˆ æ··æ·†çŸ©é˜µ (è¡Œç™¾åˆ†æ¯”):")
    print(" " * 8, end="")
    for name in class_names:
        print(f"{name:>8}", end="")
    print("  (é¢„æµ‹)")
    
    for i in range(n_classes):
        row_sum = cm[i, :].sum()
        print(f"{class_names[i]:>8}", end="")
        for j in range(n_classes):
            percentage = (cm[i, j] / row_sum * 100) if row_sum > 0 else 0
            print(f"{percentage:>7.1f}%", end="")
        print()
        
def analyze_misclassifications(misclassified, class_names=None, total_samples_per_class=None):
    """åˆ†æé”™è¯¯åˆ†ç±»æ¨¡å¼
    
    Args:
        misclassified: é”™è¯¯åˆ†ç±»çš„æ ·æœ¬åˆ—è¡¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        total_samples_per_class: æ¯ä¸ªç±»åˆ«çš„æ€»æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼‰
    """
    if not misclassified:
        print("ğŸ‰ æ²¡æœ‰é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ï¼")
        return
    
    # è·å–ç±»åˆ«æ•°é‡
    n_classes = max(max(mc['true_label'] for mc in misclassified), 
                   max(mc['predicted_label'] for mc in misclassified)) + 1
    
    if class_names is None:
        class_names = [f'Class {i}' for i in range(n_classes)]
    
    # ç»Ÿè®¡é”™è¯¯åˆ†ç±»æ¨¡å¼
    error_patterns = {}
    for mc in misclassified:
        pattern = (mc['true_label'], mc['predicted_label'])
        error_patterns[pattern] = error_patterns.get(pattern, 0) + 1
    
    print(f"\nğŸ” é”™è¯¯åˆ†ç±»åˆ†æ (å…± {len(misclassified)} ä¸ªé”™è¯¯æ ·æœ¬):")
    print("=" * 60)
    
    # æŒ‰é¢‘ç‡æ’åº
    sorted_patterns = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)
    
    for (true_label, pred_label), count in sorted_patterns:
        true_name = class_names[true_label]
        pred_name = class_names[pred_label]
        print(f"  {true_name} â†’ {pred_name}: {count} æ¬¡")
    
    # åˆ†ææ¯ä¸ªç±»åˆ«çš„é”™è¯¯æƒ…å†µ
    print(f"\nğŸ“‹ å„ç±»åˆ«é”™è¯¯ç»Ÿè®¡:")
    for i in range(n_classes):
        # ç»Ÿè®¡çœŸå®æ ‡ç­¾ä¸ºiçš„é”™è¯¯æ ·æœ¬
        class_errors = [mc for mc in misclassified if mc['true_label'] == i]
        
        if total_samples_per_class is not None and i < len(total_samples_per_class):
            # å¦‚æœæä¾›äº†æ¯ä¸ªç±»åˆ«çš„æ€»æ ·æœ¬æ•°ï¼Œä½¿ç”¨å‡†ç¡®çš„è®¡ç®—
            total_class_samples = total_samples_per_class[i]
            error_rate = len(class_errors) / total_class_samples if total_class_samples > 0 else 0
            print(f"  {class_names[i]}: {len(class_errors)} ä¸ªé”™è¯¯, é”™è¯¯ç‡: {error_rate:.2%}")
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ€»æ ·æœ¬æ•°ï¼Œåªæ˜¾ç¤ºé”™è¯¯æ•°é‡
            print(f"  {class_names[i]}: {len(class_errors)} ä¸ªé”™è¯¯")
        
        # è¿™ä¸ªç±»åˆ«æœ€å¸¸è¢«è¯¯åˆ†ä¸ºå“ªäº›ç±»åˆ«
        if class_errors:
            pred_counts = {}
            for mc in class_errors:
                pred_counts[mc['predicted_label']] = pred_counts.get(mc['predicted_label'], 0) + 1
            
            if pred_counts:
                most_common = max(pred_counts.items(), key=lambda x: x[1])
                print(f"    æœ€å¸¸è¯¯åˆ†ä¸º: {class_names[most_common[0]]} ({most_common[1]} æ¬¡)")
    
    # æ˜¾ç¤ºå…·ä½“çš„é”™è¯¯åˆ†ç±»æ ·æœ¬è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰
    print(f"\nğŸ” å…·ä½“é”™è¯¯åˆ†ç±»æ ·æœ¬ä¿¡æ¯:")
    print("=" * 60)
    for i, mc in enumerate(misclassified): 
        true_name = class_names[mc['true_label']]
        pred_name = class_names[mc['predicted_label']]
        print(f"æ ·æœ¬ #{i+1}:")
        print(f"  â€¢ æ ·æœ¬ç´¢å¼•: {mc['sample_index']}")
        print(f"  â€¢ æ‰¹æ¬¡ä¿¡æ¯: {mc['batch_info']}")
        print(f"  â€¢ çœŸå®ç±»åˆ«: {true_name} ")
        print(f"  â€¢ é¢„æµ‹ç±»åˆ«: {pred_name} ")
        print(f"  â€¢ é¢„æµ‹ç½®ä¿¡åº¦: {mc['confidence']:.4f}")
        print(f"  â€¢ çœŸå®ç±»åˆ«ç½®ä¿¡åº¦: {mc['true_class_confidence']:.4f}")
        print(f"  â€¢ ç½®ä¿¡åº¦å·®å¼‚: {mc['confidence'] - mc['true_class_confidence']:.4f}")
        
        # æ˜¾ç¤ºæ ·æœ¬æ¥æºä¿¡æ¯
        if mc['sample_info']:
            info = mc['sample_info']
            print(f"  â€¢ æ¥æºæ–‡ä»¶: {info['csv_file']}")
            print(f"  â€¢ ç±»åˆ«æ–‡ä»¶å¤¹: {info['class_name']}")
        print()
    
         
if __name__ == "__main__":
    DATA_PATH = r"F:\åºåˆ—\datacon2020_augmented"  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
    
    # å®Œæ•´é…ç½®ç¤ºä¾‹
    CUSTOM_CONFIG = {
        'train_ratio': 0.9,  # ä»trainæ–‡ä»¶å¤¹ä¸­åˆ’åˆ†90%ä½œä¸ºè®­ç»ƒé›†
        'val_ratio': 0.1,     # ä»trainæ–‡ä»¶å¤¹ä¸­åˆ’åˆ†10%ä½œä¸ºéªŒè¯é›†
        'min_samples': 10,   # å¯è‡ªå®šä¹‰æœ€å°æ ·æœ¬æ•°
        'patience': 5,  # è®¾ç½®5è½®è€å¿ƒå€¼
        'save_best_model': True,
        'use_amp': True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆGPUåŠ é€Ÿï¼‰
        'num_workers': 4  # DataLoaderå·¥ä½œè¿›ç¨‹æ•°
    }
    
    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®å¯åŠ¨è®­ç»ƒå’Œè¯„ä¼°
    train_model(DATA_PATH, split_params=CUSTOM_CONFIG)
